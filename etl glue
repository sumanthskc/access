import sys
from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql.functions import split, col, input_file_name, when

# =========================================================
# INIT
# =========================================================
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'PROCESS_DATE'])
PROCESS_DATE = args['PROCESS_DATE']

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

# =========================================================
# THE MAGIC SPEED FIX FOR 1000+ SMALL GZ FILES
# Forces standard Spark to group tiny .gz files into chunks 
# before processing, preventing the 1-hour delay.
# =========================================================
spark.conf.set("spark.sql.files.maxPartitionBytes", "134217728")
spark.conf.set("spark.sql.files.openCostInBytes", "134217728")
spark.conf.set("spark.sql.parquet.compression.codec", "snappy")

INPUT_PATH = f"s3://ist-network-intelligence-vpcflowlogs/AWSLogs/898302938380/vpcflowlogs/*/{PROCESS_DATE}/*.gz"
OUTPUT_PATH = "s3://ist-network-intelligence-vpcflowlogs-curated/testing_area/"

# =========================================================
# 1. READ WITH DYNAMIC HEADERS (The Fix for Data Shifting)
# This reads the first line of the .gz files to get the exact format.
# =========================================================
raw_df = spark.read.option("header", "true").option("sep", " ").csv(INPUT_PATH)

if raw_df.isEmpty():
    raise Exception(f"❌ ERROR: No files found for path: {INPUT_PATH}")

df = raw_df

# =========================================================
# 2. CLEAN UP COLUMN NAMES
# AWS headers use dashes (e.g., 'account-id'). Athena hates dashes.
# This converts them to underscores (e.g., 'account_id').
# =========================================================
for c in df.columns:
    clean_name = c.replace("-", "_").lower()
    df = df.withColumnRenamed(c, clean_name)

# =========================================================
# 3. NORMALIZE "-" TO NULL & RENAME RESERVED WORDS
# =========================================================
for c in df.columns:
    df = df.withColumn(c, when(col(c) == "-", None).otherwise(col(c)))

# Athena reserves 'start' and 'end', so we rename them safely
if "start" in df.columns:
    df = df.withColumnRenamed("start", "start_time")
if "end" in df.columns:
    df = df.withColumnRenamed("end", "end_time")

# =========================================================
# 4. DYNAMIC CASTING TO MATCH ATHENA
# Only cast if the column actually exists in your custom log
# =========================================================
int_cols = ["version", "srcport", "dstport", "protocol", "tcp_flags", "traffic_path"]
bigint_cols = ["packets", "bytes", "start_time", "end_time"]

for c in int_cols:
    if c in df.columns:
        df = df.withColumn(c, col(c).cast("integer"))

for c in bigint_cols:
    if c in df.columns:
        df = df.withColumn(c, col(c).cast("bigint"))

# =========================================================
# 5. EXTRACT PARTITIONS FROM PATH
# =========================================================
path = split(input_file_name(), "/")
df = (
    df
    .withColumn("region", path.getItem(6))
    .withColumn("year",   path.getItem(7))
    .withColumn("month",  path.getItem(8))
    .withColumn("day",    path.getItem(9))
)

# =========================================================
# 6. WRITE PARQUET
# =========================================================
# Limiting to 10000 for a quick test run.
df_final = df.limit(10000).repartition(2)

(
    df_final.write
      .mode("overwrite")
      .partitionBy("region", "year", "month", "day")
      .parquet(OUTPUT_PATH)
)

print(f"✅ Run completed successfully for {PROCESS_DATE}")
