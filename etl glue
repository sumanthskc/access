import sys
from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql.functions import split, col, input_file_name, when

# =========================================================
# INIT
# =========================================================
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'PROCESS_DATE'])
PROCESS_DATE = args['PROCESS_DATE']

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

# =========================================================
# PRODUCTION PERFORMANCE TUNING
# =========================================================
spark.conf.set("spark.sql.files.maxPartitionBytes", "134217728")
spark.conf.set("spark.sql.files.openCostInBytes", "134217728")
spark.conf.set("spark.sql.parquet.compression.codec", "snappy")

# SAFE RERUNS: Ensures overwriting only affects the specific date processed
spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")

# =========================================================
# PATHS (PRODUCTION)
# =========================================================
# CHANGED: Replaced the hardcoded account ID with a '*' wildcard 
# so it automatically picks up multiple accounts.
INPUT_PATH = f"s3://ist-network-intelligence-vpcflowlogs/AWSLogs/*/vpcflowlogs/*/{PROCESS_DATE}/*.gz"

# Pointed to the final curated bucket
OUTPUT_PATH = "s3://ist-network-intelligence-vpcflowlogs-curated/vpcflowlogs/"

# =========================================================
# 1. READ WITH DYNAMIC HEADERS
# =========================================================
raw_df = spark.read.option("header", "true").option("sep", " ").csv(INPUT_PATH)

if raw_df.isEmpty():
    print(f"⚠️ WARNING: No files found for {PROCESS_DATE}. Exiting gracefully.")
    sys.exit(0)

df = raw_df

# =========================================================
# 2. CLEAN UP COLUMN NAMES
# =========================================================
for c in df.columns:
    clean_name = c.replace("-", "_").lower()
    df = df.withColumnRenamed(c, clean_name)

# =========================================================
# 3. NORMALIZE "-" TO NULL & RENAME RESERVED WORDS
# =========================================================
for c in df.columns:
    df = df.withColumn(c, when(col(c) == "-", None).otherwise(col(c)))

if "start" in df.columns:
    df = df.withColumnRenamed("start", "start_time")
if "end" in df.columns:
    df = df.withColumnRenamed("end", "end_time")

# =========================================================
# 4. DYNAMIC CASTING TO MATCH ATHENA
# =========================================================
int_cols = ["version", "srcport", "dstport", "protocol", "tcp_flags", "traffic_path"]
bigint_cols = ["packets", "bytes", "start_time", "end_time"]

for c in int_cols:
    if c in df.columns:
        df = df.withColumn(c, col(c).cast("integer"))

for c in bigint_cols:
    if c in df.columns:
        df = df.withColumn(c, col(c).cast("bigint"))

# =========================================================
# 5. EXTRACT PARTITIONS FROM PATH
# =========================================================
# CHANGED: Added account_id extraction from the S3 path string
path = split(input_file_name(), "/")
df = (
    df
    .withColumn("account_id", path.getItem(4))
    .withColumn("region",     path.getItem(6))
    .withColumn("year",       path.getItem(7))
    .withColumn("month",      path.getItem(8))
    .withColumn("day",        path.getItem(9))
)

# =========================================================
# 6. WRITE PARQUET (PRODUCTION SIZING)
# =========================================================
# CHANGED: Added account_id to the repartitioning and partitionBy logic
df_final = df.repartition(200, "account_id", "region", "year", "month", "day")

(
    df_final.write
      .mode("overwrite") 
      .partitionBy("account_id", "region", "year", "month", "day")
      .parquet(OUTPUT_PATH)
)

print(f"✅ PRODUCTION Run completed successfully for {PROCESS_DATE}")
