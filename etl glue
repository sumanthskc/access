import sys
from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql.functions import split, col, input_file_name, when

# =========================================================
# INIT
# =========================================================
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'PROCESS_DATE'])
PROCESS_DATE = args['PROCESS_DATE']

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

# =========================================================
# PRODUCTION PERFORMANCE TUNING
# =========================================================
# 1. Group small files to prevent 1000+ tiny tasks
spark.conf.set("spark.sql.files.maxPartitionBytes", "134217728")
spark.conf.set("spark.sql.files.openCostInBytes", "134217728")
spark.conf.set("spark.sql.parquet.compression.codec", "snappy")

# 2. Safe Reruns: Overwrite only the specific partition being processed
spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")

# 3. S3 Read/Write Speed Fixes (Crucial for 40GB+ of data)
spark.conf.set("mapreduce.input.fileinputformat.list-status.num-threads", "100")
spark.conf.set("spark.sql.parquet.fs.optimized.committer.optimization-enabled", "true")

# =========================================================
# PATHS
# =========================================================
INPUT_PATH = f"s3://ist-network-intelligence-vpcflowlogs/AWSLogs/*/vpcflowlogs/*/{PROCESS_DATE}/*.gz"
OUTPUT_PATH = "s3://ist-network-intelligence-vpcflowlogs-curated/vpcflowlogs/"

# =========================================================
# 1. READ WITH DYNAMIC HEADERS
# =========================================================
raw_df = spark.read.option("header", "true").option("sep", " ").csv(INPUT_PATH)

if raw_df.isEmpty():
    print(f"⚠️ WARNING: No files found for {PROCESS_DATE}. Exiting gracefully.")
    sys.exit(0)

df = raw_df

# =========================================================
# 2. CLEAN UP COLUMN NAMES
# =========================================================
for c in df.columns:
    clean_name = c.replace("-", "_").lower()
    df = df.withColumnRenamed(c, clean_name)

# =========================================================
# 3. NORMALIZE "-" TO NULL & RENAME RESERVED WORDS
# =========================================================
for c in df.columns:
    df = df.withColumn(c, when(col(c) == "-", None).otherwise(col(c)))

if "start" in df.columns:
    df = df.withColumnRenamed("start", "start_time")
if "end" in df.columns:
    df = df.withColumnRenamed("end", "end_time")

# =========================================================
# 4. DYNAMIC CASTING TO MATCH ATHENA
# =========================================================
int_cols = ["version", "srcport", "dstport", "protocol", "tcp_flags", "traffic_path"]
bigint_cols = ["packets", "bytes", "start_time", "end_time"]

for c in int_cols:
    if c in df.columns:
        df = df.withColumn(c, col(c).cast("integer"))

for c in bigint_cols:
    if c in df.columns:
        df = df.withColumn(c, col(c).cast("bigint"))

# =========================================================
# 5. EXTRACT PARTITIONS FROM PATH
# =========================================================
path = split(input_file_name(), "/")
df = (
    df
    .withColumn("account_id", path.getItem(4))
    .withColumn("region",     path.getItem(6))
    .withColumn("year",       path.getItem(7))
    .withColumn("month",      path.getItem(8))
    .withColumn("day",        path.getItem(9))
)

# =========================================================
# 6. WRITE PARQUET (ANTI-SKEW LOGIC)
# =========================================================
# By repartitioning with a flat number (400) instead of column names, 
# Spark distributes the 200GB+ of raw data perfectly evenly across all workers.
df_final = df.repartition(400)

(
    df_final.write
      .mode("overwrite") 
      .partitionBy("account_id", "region", "year", "month", "day")
      .option("maxRecordsPerFile", 2_000_000) # Prevents "Small File Syndrome" in S3
      .parquet(OUTPUT_PATH)
)

print(f"✅ PRODUCTION Run completed successfully for {PROCESS_DATE}")
