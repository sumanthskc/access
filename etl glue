import sys
from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql.functions import split, col, input_file_name, when

# =========================================================
# INIT
# =========================================================
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'PROCESS_DATE'])
PROCESS_DATE = args['PROCESS_DATE']

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

# =========================================================
# THE MAGIC SPEED FIX FOR 1000+ SMALL GZ FILES
# This forces standard Spark to group tiny .gz files into 128MB chunks 
# before processing, preventing the 1-hour delay.
# =========================================================
spark.conf.set("spark.sql.files.maxPartitionBytes", "134217728")
spark.conf.set("spark.sql.files.openCostInBytes", "134217728")
spark.conf.set("spark.sql.parquet.compression.codec", "snappy")

# =========================================================
# PATHS
# =========================================================
INPUT_PATH = (
    "s3://ist-network-intelligence-vpcflowlogs/"
    f"AWSLogs/898302938380/vpcflowlogs/*/{PROCESS_DATE}/*.gz"
)

OUTPUT_PATH = "s3://ist-network-intelligence-vpcflowlogs-curated/testing_area/"

# =========================================================
# READ GZ FILES
# =========================================================
raw_df = spark.read.text(INPUT_PATH)

# Safety check so it doesn't fail silently if the date is wrong
if raw_df.isEmpty():
    raise Exception(f"❌ ERROR: No files found for path: {INPUT_PATH}")

# =========================================================
# VPC FLOW LOG SCHEMA & PARSING
# =========================================================
flow_log_columns = [
    "version", "log_account_id", "interface_id", "srcaddr", "dstaddr",
    "srcport", "dstport", "protocol", "packets", "bytes", "start", "end",
    "action", "log_status", "vpc_id", "subnet_id", "instance_id",
    "tcp_flags", "type", "pkt_srcaddr", "pkt_dstaddr", "log_region",
    "az_id", "sublocation_type", "sublocation_id", "pkt_src_aws_service",
    "pkt_dst_aws_service", "flow_direction", "traffic_path"
]

df = raw_df.withColumn("parts", split(col("value"), " "))

for idx, name in enumerate(flow_log_columns):
    df = df.withColumn(name, col("parts").getItem(idx))

df = df.drop("value", "parts")

# =========================================================
# THE FIX FOR NULL COLUMNS
# VPC logs have a header row (e.g., "version account-id interface-id...").
# If we try to cast the word "bytes" to a bigint, Spark turns it into NULL.
# This filter keeps ONLY rows where 'version' is a number.
# =========================================================
df = df.filter(col("version").rlike("^[0-9]+$"))

# =========================================================
# NORMALIZE "-" → NULL & CAST
# =========================================================
for c in flow_log_columns:
    df = df.withColumn(c, when(col(c) == "-", None).otherwise(col(c)))

numeric_cols = [
    "srcport", "dstport", "protocol", "packets", "bytes",
    "start", "end", "tcp_flags", "traffic_path"
]

for c in numeric_cols:
    df = df.withColumn(c, col(c).cast("bigint"))

# =========================================================
# EXTRACT PARTITIONS FROM FILE PATH
# =========================================================
path = split(input_file_name(), "/")
df = (
    df
    .withColumn("account_id", path.getItem(4))
    .withColumn("region",     path.getItem(6))
    .withColumn("year",       path.getItem(7))
    .withColumn("month",      path.getItem(8))
    .withColumn("day",        path.getItem(9))
)

# =========================================================
# TEST WRITE (Limiting to 10,000 rows for quick testing)
# =========================================================
df_test = df.limit(10000).repartition(2)

(
    df_test.write
      .mode("overwrite")
      .partitionBy("region", "year", "month", "day")
      .parquet(OUTPUT_PATH)
)

print(f"✅ Test run completed successfully for {PROCESS_DATE}")
